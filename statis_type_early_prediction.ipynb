{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "statis_type_early_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChNZQfL2y7Q",
        "colab_type": "text"
      },
      "source": [
        "#Predicting type of Stasis based on GitHub data\n",
        "\n",
        "The notebook contains the source code for predicting different types of stasis\n",
        "1. The data for two projects is also provided along with the notebook\n",
        "2. The code has two parts - the first part processes the data and generates labels - non-stasis, non-approval, non-convergence and non-contribution\n",
        "3. The second part trains the model that generates multiple datapoints for each issue in GitHub\n",
        "4. The model is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZzC68zHITZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "from datetime import datetime\n",
        "##Processing all actions of Development process\n",
        "MY_WORKSPACE_DIR = \"/content/drive/My Drive/DNT_Data/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izOt84XO31cm",
        "colab_type": "text"
      },
      "source": [
        "## Processing issue data in github\n",
        "1. Adds additional information such as time since start, time since creation of the issue\n",
        "2. Adds event number for each contribution\n",
        "3. Filters all contribution after the close of an issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZbnUBlrIZrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#read the nodes\n",
        "node_file = MY_WORKSPACE_DIR + 'issue_devdata_flask.csv' # of issue_dev_data_rasa.csv\n",
        "timestamp_col='date'\n",
        "dateparse = lambda x: datetime.strptime(str(x), '%d/%m/%Y %H:%M')\n",
        "\n",
        "df_dev = pd.read_csv(node_file ,parse_dates=[timestamp_col], date_parser=dateparse)\n",
        "df_dev[timestamp_col] = pd.to_datetime(df_dev[timestamp_col])\n",
        "\n",
        "def extract_timestamp_features(group):\n",
        "    \n",
        "    group = group.sort_values(timestamp_col, ascending=False, kind='mergesort')\n",
        "    \n",
        "    tmp = group[timestamp_col] - group[timestamp_col].shift(-1)\n",
        "    tmp.fillna(pd.Timedelta(seconds=0),inplace=True)\n",
        "    group[\"timesincelast\"] = tmp.apply(lambda x: float(x / np.timedelta64(1, 'D'))) # D for days\n",
        "    \n",
        "\n",
        "    tmp = group[timestamp_col] - group[timestamp_col].iloc[-1]\n",
        "    tmp=tmp.fillna(pd.Timedelta(seconds=0))\n",
        "    group[\"timesincestart\"] = tmp.apply(lambda x: float(x / np.timedelta64(1, 'D'))) # D for days\n",
        "    #tmp=tmp.fillna(pd.Timedelta(seconds=0),inplace=True)\n",
        "\n",
        "    group = group.sort_values(timestamp_col, ascending=True, kind='mergesort')\n",
        "    group[\"event_nr\"] = range(1, len(group) + 1)\n",
        "    \n",
        "    return group\n",
        "\n",
        "relevant_action='closed'\n",
        "def cut_before_action(group):\n",
        "    relevant_act_idxs = np.where(group['action'] == relevant_action)[0]\n",
        "    if len(relevant_act_idxs) > 0:\n",
        "        cut_idx = relevant_act_idxs[0]\n",
        "        return group[:cut_idx+1]\n",
        "    else:\n",
        "        return group\n",
        "\n",
        "\n",
        "\n",
        "df_dev['userid'] = df_dev['userid'].fillna('NA')\n",
        "df_dev.index.name=None#\n",
        "df_dev.reset_index(inplace=True, drop=True)\n",
        "df_dev = df_dev.groupby('id').apply(extract_timestamp_features)\n",
        "\n",
        "df_dev.index.name=None\n",
        "df_dev.reset_index(inplace=True, drop=True)\n",
        "df_dev = df_dev.sort_values(['id',timestamp_col], kind='mergesort').groupby('id').apply(cut_before_action)\n",
        "df_dev.index.name=None\n",
        "df_dev.reset_index(inplace=True, drop=True)\n",
        "print(df_dev.groupby('id').count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjoQaZb74gBt",
        "colab_type": "text"
      },
      "source": [
        "### Identify the processing time for each issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDL6EiE98TH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labeling functions for non-convergence, non-approval, non-contribution\n",
        "df_dev = df_dev.sort_values([timestamp_col], ascending=True, kind='mergesort')\n",
        "dt_first_last_timestamps = df_dev.groupby('id')[timestamp_col].agg([min, max])\n",
        "dt_first_last_timestamps.columns = [\"start_time\", \"end_time\"]\n",
        "dt_first_last_timestamps['total_time'] = dt_first_last_timestamps['end_time']-dt_first_last_timestamps['start_time']\n",
        "dt_first_last_timestamps.drop(['start_time', 'end_time'], axis=1)\n",
        "dt_first_last_timestamps['total_time'] =dt_first_last_timestamps['total_time'].dt.days\n",
        "#dt_first_last_timestamps['log_time'] = np.log(dt_first_last_timestamps['total_time'])\n",
        "df_dev = df_dev.merge(dt_first_last_timestamps['total_time'], on='id', how='left')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwUxxWrrBra6",
        "colab_type": "text"
      },
      "source": [
        "### Create labeling functions\n",
        "\n",
        "We use the following parameters \n",
        "Issues that take > median time for closure have some form of stasis - by default it is considered non-approval\n",
        "1. less # of contributors and # number of contributions - non-contribution\n",
        "2. High # of contributors and # number of contributions - non-convergence\n",
        "3. Time taken to merge a pull-request or close the issue - non-approval\n",
        "4. Store the file and then use it for the next stage of prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n07Vzl4YIeY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(df_dev.groupby('id').count())\n",
        "df_dev= df_dev[df_dev['total_time']>1]\n",
        "print(df_dev.groupby('id').count())\n",
        "ttime = df_dev.quantile([0.50,0.75])['total_time']\n",
        "print(ttime)\n",
        "\n",
        "df_num_actions = df_dev.groupby('id').size().reset_index(name='counts')\n",
        "#df_num_actions.columns=['num_actions']\n",
        "df_contributors =  df_dev.groupby('id')['userid'].agg(['nunique'])\n",
        "df_contributors.columns=['num_users']\n",
        "df_contributors = df_contributors.reset_index()\n",
        "#print(df_contributors)\n",
        "#contributors = df_contributors['num_users'].quantile([0.25,0.75])\n",
        "#actions = df_num_actions['counts'].quantile([0.5,0.75])\n",
        "#low_act=actions.iloc[0]\n",
        "#high_act=actions.iloc[1]\n",
        "#low_contrib = contributors.iloc[0]\n",
        "#high_contrib = contributors.iloc[1]\n",
        "#print(low_act, high_act, low_contrib, high_contrib)\n",
        "\n",
        "df_dev.reset_index(inplace=True, drop=True)\n",
        "df_dev = df_dev.merge(df_num_actions[['id','counts']], on='id', how='outer')\n",
        "df_dev = df_dev.merge(df_contributors[['id','num_users']], on='id',  how='outer')\n",
        "\n",
        "df_dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHTVi7_5TSYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dev['approve_date'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPT2CrgQ-wY",
        "colab_type": "text"
      },
      "source": [
        "### Generate the pre-processed file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiDufZ_LXnHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assign class label\n",
        "#df_dev=df_dev.to_frame()\n",
        "low_contrib = 3\n",
        "low_act=5\n",
        "high_contrib = 5\n",
        "high_act=12\n",
        "\n",
        "\n",
        "df_dev[\"class_label\"] = df_dev['total_time'].apply(lambda x: 1 if x >75 else 0)\n",
        "\n",
        "df_dev['class_label'].loc[ (df_dev['approve_date']>=15)] = 1 #non-approval\n",
        "\n",
        "df_dev['class_label'].loc[ (df_dev['counts']<=low_act) & (df_dev['num_users']<=low_contrib) & (df_dev['class_label']==1)] = 2  # non-contribution\n",
        "df_dev.groupby('class_label').count()\n",
        "df_dev['class_label'].loc[ (df_dev['counts']>=high_act) & (df_dev['num_users']>=high_contrib) & (df_dev['class_label']==1)] = 3 # non-convergence\n",
        "\n",
        "\n",
        "print(df_dev.groupby('class_label').count())\n",
        "print(df_dev.groupby('id').count())\n",
        "\n",
        "#df_dev\n",
        "df_dev.to_csv(os.path.join(MY_WORKSPACE_DIR, \"process_devprocess_flask.csv\"), sep=\";\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51t-J02hN2Vu",
        "colab_type": "text"
      },
      "source": [
        "### Utility functions \n",
        "1. Read the data file\n",
        "2. Add checkpoint data\n",
        "3. Generate sub-execution data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqRZ0LTopNI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "def read_dataset(filename):\n",
        "# read dataset\n",
        "  cat_cols=['userid','action','date']\n",
        "  dtypes = {col:\"object\" for col in cat_cols}\n",
        "  for col in ['timesincelast','timesincestart','event_nr','total_time']:\n",
        "    dtypes[col] = \"float\"\n",
        "  data = pd.read_csv(filename, sep=\";\", dtype=dtypes)\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "  return data\n",
        "\n",
        "num_days=1\n",
        "def add_checkpoint(group):\n",
        "    \n",
        "    frow =group.iloc[[-1]]\n",
        "    last_event_date = frow.date.copy()\n",
        "    frow.action='check_point'\n",
        "    frow.date= group.iloc[0].date + timedelta(days=num_days)\n",
        "    frow.timesincestart=num_days\n",
        "    tsincelast = float((frow.date - last_event_date)/ np.timedelta64(1, 'D'))\n",
        "    frow.timesincelast= tsincelast\n",
        "    group=group.append(frow)\n",
        "    return group\n",
        "\n",
        "def generate_subcontribution_data(data, min_length, max_length, time_span=9):\n",
        "    # generate sub contribution data (each possible contrib becomes a trace)\n",
        "    data['case_length'] = data.groupby('id')['action'].transform(len)\n",
        "    dt_contribs = data[data['case_length'] >= min_length].groupby('id').head(min_length)\n",
        "    dt_contribs[\"issue_nr\"] = 1\n",
        "    dt_contribs[\"orig_id\"] = dt_contribs['id']\n",
        "    for nr_days in range(min_length+time_span, max_length+1, time_span):\n",
        "      tmp = data[data['timesincestart'] <= nr_days].groupby('id').head(nr_days)\n",
        "      tmp.reset_index()\n",
        "      tmp[\"orig_id\"] = tmp['id']\n",
        "      tmp['id'] = tmp['id'].apply(lambda x: \"%s_%s\"%(x, nr_days))\n",
        "      tmp[\"issue_nr\"] = nr_days\n",
        "      global num_days\n",
        "      num_days = nr_days\n",
        "      print(num_days)\n",
        "      tmp = tmp.groupby('id').apply(add_checkpoint)\n",
        "      dt_contribs = pd.concat([dt_contribs, tmp], axis=0)\n",
        "      \n",
        "        \n",
        "    dt_contribs['case_length'] = dt_contribs['case_length'].apply(lambda x: min(max_length, x))\n",
        "        \n",
        "    return dt_contribs\n",
        "\n",
        "def split_data(data, train_ratio, split=\"temporal\", seed=20):  \n",
        "        # split into train and test using temporal split\n",
        "\n",
        "    grouped = data.groupby('id')\n",
        "    start_timestamps = grouped['date'].min().reset_index()\n",
        "    if split == \"temporal\":\n",
        "        start_timestamps = start_timestamps.sort_values('date', ascending=True, kind=\"mergesort\")\n",
        "    elif split == \"random\":\n",
        "        np.random.seed(seed)\n",
        "        start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
        "    train_ids = list(start_timestamps['id'])[:int(train_ratio*len(start_timestamps))]\n",
        "    train = data[data['id'].isin(train_ids)].sort_values('date', ascending=True, kind='mergesort')\n",
        "    test = data[~data['id'].isin(train_ids)].sort_values('date', ascending=True, kind='mergesort')\n",
        "\n",
        "    return (train, test)\n",
        "\n",
        "def get_label(data):\n",
        "    return data.groupby('id').first()['class_label']\n",
        "\n",
        "def get_issue_lengths(data):\n",
        "        return data.groupby('id').last()[\"issue_nr\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wtPV-ocQrwk",
        "colab_type": "text"
      },
      "source": [
        "### Feature encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3V8eG10vcD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder to encode it using aggregation - you may end up with too many resources.\n",
        "\n",
        "from sklearn.base import TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "import sys\n",
        "\n",
        "class AggregateTransformer(TransformerMixin):\n",
        "    \n",
        "    def __init__(self, case_id_col, cat_cols, num_cols, boolean=False, fillna=True):\n",
        "        self.case_id_col = case_id_col\n",
        "        self.cat_cols = cat_cols\n",
        "        self.num_cols = num_cols\n",
        "        \n",
        "        self.boolean = boolean\n",
        "        self.fillna = fillna\n",
        "        \n",
        "        self.columns = None\n",
        "        \n",
        "        self.fit_time = 0\n",
        "        self.transform_time = 0\n",
        "    \n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        start = time()\n",
        "        \n",
        "        \n",
        "        dt_first = X.groupby('id').first()\n",
        "        \n",
        "        # add the num_actions and num_users\n",
        "        dt_contrib = X.groupby(self.case_id_col).size()\n",
        "        dt_contrib.columns=['actcount']\n",
        "       \n",
        "        \n",
        "\n",
        "        # transform numeric cols\n",
        "        if len(self.num_cols) > 0:\n",
        "            dt_numeric = X.groupby(self.case_id_col)[self.num_cols].agg(['mean', 'max', 'sum'])\n",
        "            dt_numeric.columns = ['_'.join(col).strip() for col in dt_numeric.columns.values]\n",
        "            \n",
        "        # transform cat cols\n",
        "        dt_transformed = pd.get_dummies(X[self.cat_cols])\n",
        "        dt_transformed[self.case_id_col] = X[self.case_id_col]\n",
        "        del X\n",
        "        if self.boolean:\n",
        "            dt_transformed = dt_transformed.groupby(self.case_id_col).max()\n",
        "        else:\n",
        "            dt_transformed = dt_transformed.groupby(self.case_id_col).sum()\n",
        "        \n",
        "        # concatenate\n",
        "        if len(self.num_cols) > 0:\n",
        "            dt_transformed = pd.concat([dt_transformed, dt_numeric], axis=1)\n",
        "            dt_transformed = pd.concat([dt_transformed, dt_contrib], axis=1)\n",
        "            del dt_numeric\n",
        "            del dt_contrib\n",
        "       \n",
        "        # fill missing values with 0-s\n",
        "        if self.fillna:\n",
        "            dt_transformed = dt_transformed.fillna(0)\n",
        "            \n",
        "        # add missing columns if necessary\n",
        "        if self.columns is None:\n",
        "            self.columns = dt_transformed.columns\n",
        "            \n",
        "        else:\n",
        "            missing_cols = [col for col in self.columns if col not in dt_transformed.columns]\n",
        "            for col in missing_cols:\n",
        "                dt_transformed[col] = 0\n",
        "            dt_transformed = dt_transformed[self.columns]\n",
        "        \n",
        "        self.transform_time = time() - start\n",
        "        return dt_transformed\n",
        "    \n",
        "    def get_feature_names(self):\n",
        "        return self.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVSktQ1dtMnM",
        "colab_type": "text"
      },
      "source": [
        "### Experiments - Predicting statis\n",
        "To run this step, the preprocessed files should have been generated\n",
        "The following steps are performed:\n",
        "1. The data is split into train and test\n",
        "2. The sub-contributions for each issue is extracted\n",
        "3. Generate the data points by encoding features\n",
        "4. Data imbalance is addressed by using SMOTE\n",
        "5. Grid search helps tune the parameters\n",
        "6. Get the best params and the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFf7s8-3SU-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from collections import defaultdict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "\n",
        "file_name = os.path.join(MY_WORKSPACE_DIR, \"process_devprocess_flask.csv\")\n",
        "data= read_dataset(file_name)\n",
        "data['userid'] = data['userid'].map(lambda x: str(x).replace('[','').replace(']',''))\n",
        "print(data.groupby('class_label').count())\n",
        "\n",
        "train_ratio = 0.70\n",
        "\n",
        "random_state = 25\n",
        "\n",
        "parameters={}\n",
        "parameters = {'learning_rate': [0.25,0.5,0.75],\n",
        "              'subsample': [0.5,0.75],\n",
        "               'max_depth': [4,6,10],\n",
        "                'colsample_bytree': [0.25,0.75],\n",
        "                'min_child_weight': [1,3,5]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train, test = split_data(data, train_ratio, split=\"temporal\")\n",
        "\n",
        "\n",
        "dt_test_contrib = generate_subcontribution_data(test, 1, 100)\n",
        "dt_train_contrib = generate_subcontribution_data(train, 1, 100)\n",
        "\n",
        "dt_train_contrib.to_csv(os.path.join(MY_WORKSPACE_DIR, \"subissues_flask.csv\"), sep=\";\", index=False)\n",
        "\n",
        "preds_all = []\n",
        "test_y_all = []\n",
        "nr_events_all = []\n",
        "\n",
        "dt_train_bucket = dt_train_contrib\n",
        "dt_test_bucket=dt_test_contrib\n",
        "train_y = get_label(dt_train_bucket)\n",
        "test_y = get_label(dt_test_contrib)\n",
        "unique, counts = np.unique(train_y, return_counts=True)\n",
        "print(unique, counts)\n",
        "unique, counts = np.unique(test_y, return_counts=True)\n",
        "print(unique, counts)\n",
        "\n",
        "\n",
        "atx=AggregateTransformer('id', ['action','userid'] , ['timesincelast', 'timesincestart'])\n",
        "\n",
        "#Flask - 0.8596004575521757\n",
        "#Flask - {'colsample_bytree': 0.75, 'learning_rate': 0.5, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.75}\n",
        "cls = xgb.XGBClassifier(objective='multi:softprob',\n",
        "                        num_class=4,\n",
        "                        n_estimators=100,\n",
        "                        learning_rate= 0.5,\n",
        "                        subsample=0.75,\n",
        "                        max_depth=10,  #10\n",
        "                        colsample_bytree=0.25,  #0.25\n",
        "                        min_child_weight=1,#1\n",
        "                        seed=random_state)\n",
        "\n",
        "\n",
        "x_train=atx.fit_transform(dt_train_bucket)\n",
        "# transform the dataset\n",
        "print('Training', x_train)\n",
        "# making sure there is data balancing\n",
        "oversample = SMOTE()\n",
        "x_new, y_new = oversample.fit_resample(x_train, train_y)\n",
        "x_newdf = pd.DataFrame(x_new, columns=x_train.columns)\n",
        "\n",
        "##########Required for grid search\n",
        "#print(x_train.columns)\n",
        "#cv = GridSearchCV(cls, parameters, scoring = 'f1_macro',cv=3, n_jobs= -1)\n",
        "#cv.fit(x_newdf, y_new)   \n",
        "#print(cv.best_score_)    \n",
        "#print(cv.best_params_) \n",
        "#########################\n",
        "#cls = cv.best_estimator_\n",
        "\n",
        "cls.fit(x_newdf, y_new)    \n",
        "test_all_grouped = dt_test_contrib.groupby('id')           \n",
        " # predict separately for each issue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqP0nV7iSdwr",
        "colab_type": "text"
      },
      "source": [
        "### process the results based on days passed since creation of issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwiUXHgSeFQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for _, group in test_all_grouped:\n",
        "    pr_len = get_issue_lengths(group)                    \n",
        "    test_y_all.extend(get_label(group))\n",
        "    x_test = atx.fit_transform(group)\n",
        "    x_testdf = pd.DataFrame(x_test, columns=atx.get_feature_names())\n",
        "    pred = cls.predict(x_testdf)\n",
        "    #print(pred)\n",
        "    preds_all.extend(pred)\n",
        "    nr_events_all.extend(pr_len)\n",
        "\n",
        "num_days = []\n",
        "f1_list = []\n",
        "f1_label=[]\n",
        "\n",
        "      \n",
        "dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
        "\n",
        "for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
        "    if len(set(group.actual)) < 2:\n",
        "        print(nr_events, \" macof1\", np.nan)\n",
        "    else:\n",
        "        score=f1_score(group.actual, np.round(group.predicted), average='weighted')\n",
        "        cls_report = classification_report(group.actual, np.round(group.predicted), output_dict=True)\n",
        "        print( nr_events, \"macrof1\", score )\n",
        "        num_days.append(nr_events)\n",
        "        f1_list.append(score)\n",
        "        f1_label.append(\"weighted\")\n",
        "        print(cls_report)\n",
        "        for label,val in cls_report.items():\n",
        "          if type(val)==dict:\n",
        "            f1_list.append(val['f1-score'])\n",
        "            f1_label.append(label)\n",
        "            num_days.append(nr_events)\n",
        "\n",
        "#print(f1_score(dt_results.actual, np.round(dt_results.predicted)), average='weighted'   )\n",
        "\n",
        "dt_plot = pd.DataFrame({\"num_days\": num_days, \"label\": f1_label, \"score\": f1_list})\n",
        "\n",
        "\n",
        "import seaborn as sns # for data visualization\n",
        "import matplotlib.pyplot as plt # for data visualization\n",
        "%matplotlib inline\n",
        " \n",
        "# Draw line plot of \n",
        "sns.lineplot(x = \"num_days\", y = \"score\", data = dt_plot, hue = \"label\",\n",
        "            style = \"label\", palette = \"hot\", dashes = False, legend=\"brief\",)\n",
        " \n",
        "plt.title(\"F1-score of prediction\", fontsize = 20) # for title\n",
        "plt.xlabel(\"Number of days\", fontsize = 15) # label for x-axis\n",
        "plt.ylabel(\"f1-score\", fontsize = 15) # label for y-axis\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}